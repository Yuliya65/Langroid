name: Pytest

on:
  push:
    paths:
      - "langroid/**"
      - "tests/**"
      - ".github/workflows/pytest.yml"
    branches:
      - main
  pull_request:
    types:
      - ready_for_review
    branches:
      - main


jobs:
  cancel:
    if: "contains(github.event.head_commit.message, 'notest') == false"
    name: Cancel Previous Runs
    runs-on: ubuntu-latest
    timeout-minutes: 3
    steps:
      - name: Cancel Previous Workflow Runs
        uses: styfle/cancel-workflow-action@a40b8845c0683271d9f53dfcb887a7e181d3918b # 0.9.1
        with:
          access_token: ${{ github.token }}
  pytest:
    if: "contains(github.event.head_commit.message, 'notest') == false"    
    runs-on: ubuntu-latest
    services:
      meilisearch:
        image: getmeili/meilisearch:latest@sha256:773759814f59214a0971dc8810ae1d85002e92166d99760cd546d5ee8ac37c14
        ports:
          - 7700:7700
        options: --env MEILI_MASTER_KEY=masterKey
      arangodb:
        image: arangodb:latest@sha256:76624eca04b8f59d7705273a2785f713a92390fee754af22566fae9d6bb8093f
        ports:
          - 8529:8529
        env:
          ARANGO_ROOT_PASSWORD: rootpassword
      neo4j:
        image: neo4j:latest@sha256:9c1ccfeace879b06069dcbea32dfb9112cdd1c7e24036f54097b7eb9880e3498
        ports:
          - 7687:7687
          - 7474:7474
        env:
          NEO4J_AUTH: neo4j/password
      weaviate:
        image: semitechnologies/weaviate:latest@sha256:9df3e25c7decba3f906829c7500129a98a21f9e8e1b816705e303c317ec6b8d5
        ports:
          - "8080:8080"
          - "50051:50051"  # Add gRPC port
        env:
          WEAVIATE_IN_MEMORY: "true"
          QUERY_DEFAULTS_LIMIT: "25"
          AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
          ENABLE_MODULES: "text2vec-openai"
          CLUSTER_HOSTNAME: "node1"
      postgres:
        image: ankane/pgvector:latest@sha256:956744bd14e9cbdf639c61c2a2a7c7c2c48a9c8cdd42f7de4ac034f4e96b90f8
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: langroid 
        ports:
          - "5432:5432"
      qdrant:
        image: qdrant/qdrant:v1.15.5@sha256:0fb8897412abc81d1c0430a899b9a81eb8328aa634e7242d1bc804c1fe8fe863
        ports:
          - "6333:6333"
        env:
          QDRANT__SERVICE__API_KEY: local-dev-key


    env:
      CI: true # allows conditional skipping of tests based on this env var
      TEST_MCP_NPX: true # allow NPX MCP tests
      OPENAI_API_KEY: ${{ secrets.OPENAI_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      GITHUB_ACCESS_TOKEN: ${{ secrets.GH_ACCESS_TOKEN }}
      REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD }}
      REDIS_HOST: ${{ secrets.REDIS_HOST }}
      REDIS_PORT: ${{ secrets.REDIS_PORT }}
      QDRANT_API_KEY: ${{ secrets.QDRANT_API_KEY }}
      QDRANT_API_URL: ${{ secrets.QDRANT_API_URL }}
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
      GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      GOOGLE_CSE_ID: ${{ secrets.GOOGLE_CSE_ID }}
      WEAVIATE_API_KEY: ${{ secrets.WEAVIATE_API_KEY }}
      WEAVIATE_API_URL: ${{ secrets.WEAVIATE_API_URL }}
      EXA_API_KEY: ${{ secrets.EXA_API_KEY }}
      TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
      LANGDB_API_KEY: ${{ secrets.LANGDB_API_KEY }}
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: langroid 

      # Azure OpenAI env vars
      AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
      AZURE_OPENAI_API_BASE: ${{ secrets.AZURE_OPENAI_API_BASE }}
      AZURE_OPENAI_API_VERSION: "2023-05-15"
      AZURE_OPENAI_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_DEPLOYMENT_NAME }}
      AZURE_OPENAI_MODEL_NAME: ${{ secrets.AZURE_GPT_MODEL_NAME }} 
      AZURE_OPENAI_MODEL_VERSION: ${{ secrets.AZURE_OPENAI_MODEL_VERSION }}
      
    steps:
    - name: Check out repository
      uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3
    - name: Increase ulimit
      run: ulimit -n 4096
    - name: Set up Python
      uses: actions/setup-python@7f4fc3e22c37d6ff65e88745f38bd3157c663f7c # v4
      with:
        python-version: 3.11  # Replace with your desired Python version

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libmagic1
        sudo apt-get install -y poppler-utils tesseract-ocr
    # - name: Restore uv cache 
    #   uses: actions/cache@v4
    #   with: 
    #     path: ~/.local
    #     key: uv-${{ runner.os }}-${{ hashFiles('uv.lock') }}
    #     restore-keys: |
    #       uv-${{ runner.os }}-${{ hashFiles('uv.lock') }}
    #       uv-${{ runner.os }}

    - name: Setup uv
      uses: astral-sh/setup-uv@d4b2f3b6ecc6e67c4457f6d3e41ec42d3d0fcb86 # v5
      with: 
        enable-cache: true
        cache-dependency-glob: "uv.lock"

    # - name: Load cached venv
    #   id: cached-uv-dependencies
    #   uses: actions/cache@v4
    #   with:
    #       path: .venv
    #       key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('uv.lock') }}
    
    - name: Install project dependencies with uv
      # if: steps.cached-uv-dependencies.outputs.cache-hit != 'true'
      run: |
        uv sync --dev \
          --extra neo4j \
          --extra arango \
          --extra lancedb \
          --extra meilisearch \
          --extra chromadb \
          --extra docling \
          --extra litellm \
          --extra pymupdf4llm \
          --extra weaviate \
          --extra pinecone \
          --extra postgres\
          --extra hf-embeddings \
          --extra unstructured \
          --extra pdf-parsers \
          --extra docx \
          --extra sql \
          --extra exa \
          --extra tavily \
          --extra firecrawl \
          --extra fastembed \
          --extra crawl4ai
        
      
    

    - name: Download ALL NLTK data (needed by unstructured and others)
      run: uv run python -c "import nltk; nltk.download('all')"

    - name: Dump dependencies to file
      run: uv pip freeze > requirements_github.txt

    - name: Upload dependencies file
      uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
      with:
        name: github-dependencies
        path: requirements_github.txt


    - name: Run ALL tests with coverage (GPT-4o-mini)
      id: first_test
      env:
        PYTEST_ADDOPTS: "-p no:logging"
      run: |
        uv run pytest --m gpt-4o-mini --cov langroid --cov-config .coveragerc \
          -rf -vv --log-cli-level=INFO --show-capture=no \
          -n auto --timeout=300 \
          --first-test-file=tests/main/sql_chat/test_sql_chat_agent.py \
          --first-test-file=tests/extras/test_hf_embeddings.py \
          --first-test-file=tests/extras/test_gemini_embeddings.py \
          --first-test-file=tests/extras/test_fastembed_embeddings.py \
          --first-test-file=tests/main/test_llm.py \
          --first-test-file=tests/main/test_task.py \
          --first-test-file=tests/main/test_llm_async.py \
          --first-test-file=tests/main/test_task_inf_loop.py \
          --first-test-file=tests/main/test_lance_doc_chat_agent.py \
          --first-test-file=tests/main/test_gemini_doc_parser.py \
          tests/main
      continue-on-error: true

    - name: Retry FAILED tests ONLY with GPT-4o instead of GPT-4o-mini
      id: second_test
      if: steps.first_test.outcome == 'failure'
      env:
        PYTEST_ADDOPTS: "-p no:logging"
      run: |
        uv run pytest --cov langroid --cov-append --cov-config .coveragerc \
          -rf -vv --log-cli-level=INFO --show-capture=no \
          -n auto --timeout=300 \
          --first-test-file=tests/main/sql_chat/test_sql_chat_agent.py \
          --first-test-file=tests/main/test_llm.py \
          --first-test-file=tests/main/test_task.py \
          --first-test-file=tests/main/test_llm_async.py \
          --first-test-file=tests/main/test_task_inf_loop.py \
          --first-test-file=tests/main/test_lance_doc_chat_agent.py \
          --lf --last-failed-no-failures none \
          tests/main tests/extras/test_hf_embeddings.py \
          tests/extras/test_fastembed_embeddings.py
      continue-on-error: true

    - name: Retry FAILED tests ONLY without cache (GPT-4o)
      id: third_test
      if: steps.second_test.outcome == 'failure'
      env:
        PYTEST_ADDOPTS: "-p no:logging"
      run: |
        uv run pytest --nc --cov langroid --cov-append --cov-config .coveragerc \
          --junitxml=test-results.xml \
          -rf --show-capture=no --timeout=300 -n auto \
          --first-test-file=tests/main/sql_chat/test_sql_chat_agent.py \
          --first-test-file=tests/main/test_llm.py \
          --first-test-file=tests/main/test_llm_async.py \
          --first-test-file=tests/main/test_task_inf_loop.py \
          --first-test-file=tests/main/test_task.py \
          --first-test-file=tests/main/test_lance_doc_chat_agent.py \
          --lf --last-failed-no-failures none \
          tests/main tests/extras/test_hf_embeddings.py \
          tests/extras/test_fastembed_embeddings.py
    - name: Generate final coverage report from all runs
      run: uv run coverage report

    - name: Generate XML coverage report
      run: uv run coverage xml
    - name: Publish Test Results
      uses: EnricoMi/publish-unit-test-result-action@34d7c956a59aed1bfebf31df77b8de55db9bbaaf # v2
      if: always()
      with:
        files: "test-results.xml"
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@b9fd7d16f6d7d1b5d2bec1a2887e65ceed900238 # v4
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
      env:
        CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
